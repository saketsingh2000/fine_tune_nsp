{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, TextDataset, DataCollatorForLanguageModeling\n",
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    " #Step 1: Generate Synthetic Data \n",
    "scenarios = [\n",
    "    {\"context\": \"Customer is asking about order tracking.\", \"agent_action\": \"Provide tracking details.\"},\n",
    "    {\"context\": \"Customer wants to know the delivery status.\", \"agent_action\": \"Check and inform about the delivery status.\"},\n",
    "    {\"context\": \"Customer wishes to cancel an order.\", \"agent_action\": \"Process the order cancellation.\"},\n",
    "    {\"context\": \"Customer asks for order replacement.\", \"agent_action\": \"Initiate the order replacement process.\"},\n",
    "    {\"context\": \"Customer inquires about returning a product.\", \"agent_action\": \"Guide the customer through the return process.\"},\n",
    "    {\"context\": \"Customer has a complaint about a product's quality.\", \"agent_action\": \"Address the complaint and offer a solution.\"},\n",
    "    {\"context\": \"Customer asks for a product recommendation.\", \"agent_action\": \"Provide product recommendations.\"},\n",
    "    {\"context\": \"Customer needs help with product setup.\", \"agent_action\": \"Assist with the product setup instructions.\"},\n",
    "    {\"context\": \"Customer is checking on warranty coverage.\", \"agent_action\": \"Provide warranty coverage information.\"},\n",
    "    {\"context\": \"Customer wants to update their contact information.\", \"agent_action\": \"Update the customer's contact details.\"},\n",
    "    {\"context\": \"Customer inquires about payment options.\", \"agent_action\": \"Explain the payment options.\"},\n",
    "    {\"context\": \"Customer is experiencing a technical issue.\", \"agent_action\": \"Offer troubleshooting steps.\"},\n",
    "    {\"context\": \"Customer wants to know about loyalty program benefits.\", \"agent_action\": \"Describe the loyalty program benefits.\"},\n",
    "    {\"context\": \"Customer asks about product recycling.\", \"agent_action\": \"Provide recycling information.\"},\n",
    "    {\"context\": \"Customer is inquiring about how to use a voucher.\", \"agent_action\": \"Explain the process of redeeming a voucher.\"},\n",
    "    {\"context\": \"Customer expresses dissatisfaction with a recent purchase.\", \"agent_action\": \"Acknowledge the issue and discuss possible resolutions.\"},\n",
    "    {\"context\": \"Customer is confused about a policy.\", \"agent_action\": \"Clarify the policy details for the customer.\"},\n",
    "    {\"context\": \"Customer received a defective product.\", \"agent_action\": \"Apologize and initiate a product exchange or return.\"},\n",
    "    {\"context\": \"Customer is asking about international shipping options.\", \"agent_action\": \"Provide information on international shipping policies and costs.\"},\n",
    "    {\"context\": \"Customer wants to change their subscription plan.\", \"agent_action\": \"Guide the customer through the process of changing their subscription.\"},\n",
    "    {\"context\": \"Customer needs assistance with resetting their password.\", \"agent_action\": \"Provide step-by-step instructions for resetting the password.\"},\n",
    "    {\"context\": \"Customer inquires about the availability of a product.\", \"agent_action\": \"Check and inform the customer about the product's availability.\"},\n",
    "    {\"context\": \"Customer is asking for an extension on a payment deadline.\", \"agent_action\": \"Discuss the possibility and process of extending the payment deadline.\"},\n",
    "    {\"context\": \"Customer wants to provide feedback on a service.\", \"agent_action\": \"Thank the customer for their feedback and guide them on how to submit it.\"},\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Customer: Customer asks about product recycling. Agent: Provide recycling information.',\n",
       " 'Customer: Customer asks for a product recommendation. Agent: Provide product recommendations.',\n",
       " 'Customer: Customer asks for order replacement. Agent: Initiate the order replacement process.',\n",
       " 'Customer: Customer expresses dissatisfaction with a recent purchase. Agent: Acknowledge the issue and discuss possible resolutions.',\n",
       " \"Customer: Customer has a complaint about a product's quality. Agent: Address the complaint and offer a solution.\",\n",
       " 'Customer: Customer inquires about payment options. Agent: Explain the payment options.',\n",
       " 'Customer: Customer inquires about returning a product. Agent: Guide the customer through the return process.',\n",
       " \"Customer: Customer inquires about the availability of a product. Agent: Check and inform the customer about the product's availability.\",\n",
       " 'Customer: Customer is asking about international shipping options. Agent: Provide information on international shipping policies and costs.',\n",
       " 'Customer: Customer is asking about order tracking. Agent: Provide tracking details.',\n",
       " 'Customer: Customer is asking for an extension on a payment deadline. Agent: Discuss the possibility and process of extending the payment deadline.',\n",
       " 'Customer: Customer is checking on warranty coverage. Agent: Provide warranty coverage information.',\n",
       " 'Customer: Customer is confused about a policy. Agent: Clarify the policy details for the customer.',\n",
       " 'Customer: Customer is experiencing a technical issue. Agent: Offer troubleshooting steps.',\n",
       " 'Customer: Customer is inquiring about how to use a voucher. Agent: Explain the process of redeeming a voucher.',\n",
       " 'Customer: Customer needs assistance with resetting their password. Agent: Provide step-by-step instructions for resetting the password.',\n",
       " 'Customer: Customer needs help with product setup. Agent: Assist with the product setup instructions.',\n",
       " 'Customer: Customer received a defective product. Agent: Apologize and initiate a product exchange or return.',\n",
       " 'Customer: Customer wants to change their subscription plan. Agent: Guide the customer through the process of changing their subscription.',\n",
       " 'Customer: Customer wants to know about loyalty program benefits. Agent: Describe the loyalty program benefits.',\n",
       " 'Customer: Customer wants to know the delivery status. Agent: Check and inform about the delivery status.',\n",
       " 'Customer: Customer wants to provide feedback on a service. Agent: Thank the customer for their feedback and guide them on how to submit it.',\n",
       " \"Customer: Customer wants to update their contact information. Agent: Update the customer's contact details.\",\n",
       " 'Customer: Customer wishes to cancel an order. Agent: Process the order cancellation.'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = set()\n",
    "for _ in range(1000):  # Generate 1000 unique samples\n",
    "    scenario = random.choice(scenarios)\n",
    "    customer_message = f\"Customer: {scenario['context']} Agent: {scenario['agent_action']}\"\n",
    "    data.add(customer_message)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the unique data to a text file\n",
    "with open('customer_service_data.txt', 'w') as f:\n",
    "    for item in data:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Fine-Tune the GPT-2 Model\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\saket.singh1\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\data\\datasets\\language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_dataset = TextDataset(\n",
    "  tokenizer=tokenizer,\n",
    "  file_path=\"customer_service_data.txt\",\n",
    "  block_size=128\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=False,\n",
    ")  #False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-customer-service\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=4,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 175/175 [11:39<00:00,  4.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 699.851, 'train_samples_per_second': 1.0, 'train_steps_per_second': 0.25, 'train_loss': 0.18299564906529017, 'epoch': 5.0}\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "trainer.save_model(\"./gpt2-customer-service\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\saket.singh1\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\saket.singh1\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "Given the following conversation, predict the next sentence:\n",
      "\n",
      "Customer: Customer is asking about order tracking. Agent: Provide tracking details.\n",
      "\n",
      "Next sentence:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - NEXT SENTENCE PREDICTION:\n",
      "Given the following conversation, predict the next sentence:\n",
      "\n",
      "Customer: Customer is asking about order tracking. Agent: Provide tracking details.\n",
      "\n",
      "Next sentence: Customer asks for order replacement. Agent: Initiate the order replacement process.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Ensure the model and tokenizer paths are correct\n",
    "model_path = \"./gpt2-customer-service\"\n",
    "tokenizer_path = \"gpt2\"\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(tokenizer_path)\n",
    "\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "def get_random_dialogue(filename):\n",
    "    try:\n",
    "        with open(filename, 'r', encoding='utf-8') as file:\n",
    "            lines = [line.strip() for line in file if line.strip()]\n",
    "            return random.choice(lines)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File {filename} not found.\")\n",
    "        return \"\"\n",
    "\n",
    "def generate_next_sentence(dialogue):\n",
    "    prompt = f\"Given the following conversation, predict the next sentence:\\n\\n{dialogue}\\n\\nNext sentence:\"\n",
    "    inputs = tokenizer(prompt, return_tensors='pt')\n",
    "    #output = model.generate(inputs[\"input_ids\"], max_length=inputs[\"input_ids\"].shape[1] + 50, pad_token_id=tokenizer.eos_token_id)\n",
    "    output = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_length=inputs[\"input_ids\"].shape[1]+ 15,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        num_return_sequences=1,\n",
    "        temperature=0.9, #0.7,\n",
    "        top_k=50,\n",
    "        top_p=0.95\n",
    "    )\n",
    "    decoded_output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return prompt, decoded_output\n",
    "\n",
    "# Get a random dialogue and generate the next sentence\n",
    "dialogue = get_random_dialogue('customer_service_data.txt')\n",
    "if dialogue:\n",
    "    prompt, output = generate_next_sentence(dialogue)\n",
    "    print('-' * 100)\n",
    "    print(f'INPUT PROMPT:\\n{prompt}')\n",
    "    print('-' * 100)\n",
    "    print(f'MODEL GENERATION - NEXT SENTENCE PREDICTION:\\n{output}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\saket.singh1\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "Given the following conversation, predict the next sentence:\n",
      "\n",
      "Customer: I want to cancel my order\n",
      "\n",
      "Next sentence:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - NEXT SENTENCE PREDICTION:\n",
      "Given the following conversation, predict the next sentence:\n",
      "\n",
      "Customer: I want to cancel my order\n",
      "\n",
      "Next sentence: Process the order cancellation.\n",
      "Customer: Customer asks for order replacement. Agent: Initiate the order replacement process.\n",
      "Customer\n"
     ]
    }
   ],
   "source": [
    "# Modified function to generate the next sentence based on a given dialogue\n",
    "def generate_next_sentence_from_specific_input(customer_input):\n",
    "    dialogue = f\"Customer: {customer_input}\"\n",
    "    prompt = f\"Given the following conversation, predict the next sentence:\\n\\n{dialogue}\\n\\nNext sentence:\"\n",
    "    inputs = tokenizer(prompt, return_tensors='pt')\n",
    "    output = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_length=inputs[\"input_ids\"].shape[1] + 25,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        num_return_sequences=1,\n",
    "        temperature=0.6,\n",
    "        top_k=50,\n",
    "        top_p=0.95\n",
    "    )\n",
    "    decoded_output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return prompt, decoded_output\n",
    "\n",
    "# Test the model with a specific customer sentence\n",
    "customer_input = \"I want to cancel my order\"\n",
    "prompt, output = generate_next_sentence_from_specific_input(customer_input)\n",
    "print('-' * 100)\n",
    "print(f'INPUT PROMPT:\\n{prompt}')\n",
    "print('-' * 100)\n",
    "print(f'MODEL GENERATION - NEXT SENTENCE PREDICTION:\\n{output}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated continuation: Customer: Customer asks for order replacement. Agent: Initiate the order replacement process.\n",
      "Customer: Customer asks for order replacement. Agent: Initiate the order replacement process.\n",
      "Customer: Customer asks for order replacement. Agent: Initiate the order\n"
     ]
    }
   ],
   "source": [
    "#trial with a different prompt\n",
    "\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "model = GPT2LMHeadModel.from_pretrained(\"./gpt2-customer-service\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "prompt = \"\"\"\n",
    "Context: This is a customer service conversation. The goal is to continue the dialogue in a coherent and contextually relevant manner.\n",
    "\n",
    "Dialogue:\n",
    "Customer: \"Customer wants to provide feedback on a service.\"\n",
    "Agent: \"Thank the customer for their feedback and guide them on how to submit it.\"\n",
    "\n",
    "Predict the next sentence in the conversation. Begin with the customer's response.\n",
    "\n",
    "Next sentence:\n",
    "\"\"\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors='pt')\n",
    "\n",
    "# Adjusting the generation parameters to encourage diversity\n",
    "output = tokenizer.decode(\n",
    "    model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_length=inputs[\"input_ids\"].shape[1] + 50,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        temperature=0.9,  # Adjust this for more randomness\n",
    "        top_k=50,  # Top-k sampling\n",
    "        top_p=0.95,  # Top-p (nucleus) sampling\n",
    "        num_return_sequences=1,\n",
    "    )[0],\n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "generated_continuation = output.split(\"Next sentence:\")[1].strip()\n",
    "print(\"Generated continuation:\", generated_continuation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
