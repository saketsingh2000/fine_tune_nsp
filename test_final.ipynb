{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer, Trainer, TrainingArguments\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data\n",
    "sample_data = [\n",
    "    {\"customer\": \"I'm having trouble logging into my account.\", \"agent_response\": \"Sure, could you please provide your email address?\"},\n",
    "    {\"customer\": \"My order hasn't arrived yet.\", \"agent_response\": \"Can you please provide your order number so I can check the status?\"},\n",
    "    {\"customer\": \"I need to update my billing information.\", \"agent_response\": \"Sure, could you please provide the new billing address?\"},\n",
    "    # Add more sample conversations here\n",
    "]\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare input-output pairs\n",
    "train_data = [{\"input_text\": conv[\"customer\"], \"target_text\": conv[\"agent_response\"]} for conv in sample_data]\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained T5 model and tokenizer\n",
    "model_name = \"t5-small\"  # or \"t5-base\", \"t5-large\", etc.\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,\n",
    "    save_steps=1000,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    output_dir='./models'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "# Define training data\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=512):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    " \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    " \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        input_text = item['input_text']\n",
    "        target_text = item['target_text']\n",
    " \n",
    "        input_encoding = self.tokenizer(input_text, truncation=True, max_length=self.max_length, padding=\"max_length\", return_tensors=\"pt\")\n",
    "        target_encoding = self.tokenizer(target_text, truncation=True, max_length=self.max_length, padding=\"max_length\", return_tensors=\"pt\")\n",
    " \n",
    "        labels = target_encoding[\"input_ids\"]\n",
    "        labels[labels == 0] = -100  # Ignore padding tokens in loss calculation\n",
    " \n",
    "        return {\n",
    "            \"input_ids\": input_encoding[\"input_ids\"].flatten(),\n",
    "            \"attention_mask\": input_encoding[\"attention_mask\"].flatten(),\n",
    "            \"labels\": labels.flatten(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(train_data, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tune the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned model\n",
    "trainer.save_model(\"./fine_tuned_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the fine-tuned model\n",
    "fine_tuned_model = T5ForConditionalGeneration.from_pretrained(\"./fine_tuned_model\")\n",
    " \n",
    "# Function to generate response\n",
    "def generate_response(input_text, model, tokenizer, max_length=50):\n",
    "    input_encoding = tokenizer(input_text, truncation=True, max_length=max_length, padding=\"max_length\", return_tensors=\"pt\")\n",
    "    input_ids = input_encoding.input_ids.to(model.device)\n",
    "    attention_mask = input_encoding.attention_mask.to(model.device)\n",
    "    output_ids = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_length=max_length, num_beams=5, early_stopping=True)\n",
    "    output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    return output_text\n",
    " \n",
    "# Test the model\n",
    "test_input = \"I want to track my order.\"\n",
    "generated_response = generate_response(test_input, fine_tuned_model, tokenizer)\n",
    "print(\"Generated response:\", generated_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
