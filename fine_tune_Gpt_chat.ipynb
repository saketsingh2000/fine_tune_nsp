{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, TextDataset, DataCollatorForLanguageModeling\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# Load data from CSV\n",
    "df = pd.read_csv(\"chatbot_data_2.csv\")\n",
    "\n",
    "# Create unique dialogues from the CSV data\n",
    "data = set()\n",
    "for _, row in df.iterrows():\n",
    "    customer_message = f\"Customer: {row['context']} Agent: {row['agent_action']}\"\n",
    "    data.add(customer_message)\n",
    "\n",
    "# Save the unique data to a text file\n",
    "with open('customer_service_data_csv.txt', 'w') as f:\n",
    "    for item in data:\n",
    "        f.write(\"%s\\n\" % item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.iterrows of                                                context  \\\n",
       "0    User: Is product X included in the current pro...   \n",
       "1    User inquires about loyalty program points ear...   \n",
       "2          User: How do I sign up for your newsletter?   \n",
       "3    User wants to dispute a charge on their credit...   \n",
       "4    User inquires about the company's social respo...   \n",
       "..                                                 ...   \n",
       "124  User wants to know about accepted payment meth...   \n",
       "125  User: I have a question about your company's p...   \n",
       "126  User: Can you provide assistance with a techni...   \n",
       "127  User asks about product compatibility with ano...   \n",
       "128  User needs help connecting a device to their W...   \n",
       "\n",
       "                                          agent_action  \n",
       "0    Let me find out for you! What promotion are yo...  \n",
       "1    Explain how points are calculated and offer op...  \n",
       "2    You can sign up for our newsletter on our webs...  \n",
       "3    Provide instructions on how to initiate a disp...  \n",
       "4    Provide information about the company's social...  \n",
       "..                                                 ...  \n",
       "124  List all available payment methods and any ass...  \n",
       "125  Sure thing! What policy do you need clarificat...  \n",
       "126  Absolutely! What technical issue are you exper...  \n",
       "127  Provide compatibility information based on pro...  \n",
       "128  Provide step-by-step instructions for Wi-Fi ne...  \n",
       "\n",
       "[129 rows x 2 columns]>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data)\n",
    "\n",
    "data\n",
    "df.iterrows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\umesh.malviya1\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\data\\datasets\\language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 3: Fine-tune the GPT-2 model\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "train_dataset = TextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=\"customer_service_data_csv.txt\",  # Path to save the preprocessed data\n",
    "    block_size=128  # Adjust as needed\n",
    ")\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=False,\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-fine-tuned\",  # Directory to save the fine-tuned model\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=5,  # Adjust as needed\n",
    "    per_device_train_batch_size=4,  # Adjust batch size based on available memory\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                              \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [03:13<00:00,  6.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 193.406, 'train_samples_per_second': 0.595, 'train_steps_per_second': 0.155, 'train_loss': 2.2226834615071613, 'epoch': 5.0}\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Step 4: Save the fine-tuned model\n",
    "trainer.save_model(\"./gpt2-fine-tuned\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\umesh.malviya1\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\umesh.malviya1\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: user order is delayed by several days and its at our delivery hub.\n",
      "Agent: Explain the delay and offer\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    " \n",
    "# Load the fine-tuned GPT-2 model and tokenizer\n",
    "model_path = \"./gpt2-fine-tuned\"\n",
    "tokenizer_path = \"gpt2\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(tokenizer_path)\n",
    " \n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    " \n",
    "# One-shot inference function\n",
    "def one_shot_inference(customer_input):\n",
    "    # Providing a single example as context\n",
    "    example_context = \"User: I want to update my address. Agent: I can help you with that. Please provide your new address.\"\n",
    "    prompt = f\"{example_context} User: {customer_input} Agent:\"\n",
    "    inputs = tokenizer(prompt, return_tensors='pt')\n",
    "    output = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_length=inputs[\"input_ids\"].shape[1] + 5,  # Adjust max_length if needed\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        num_return_sequences=1,\n",
    "        temperature=0.9,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        no_repeat_ngram_size=2  # Prevents repeating n-grams\n",
    "    )\n",
    "    decoded_output = tokenizer.decode(output[0], skip_special_tokens=True).replace(prompt, '').strip()\n",
    "    decoded_output = decoded_output.split(\"User:\")[0].strip()\n",
    "    return decoded_output\n",
    " \n",
    "# Example usage\n",
    "customer_input = \"user order is delayed by several days and its at our delivery hub.\"\n",
    "agent_action = one_shot_inference(customer_input)\n",
    "print(f'User: {customer_input}')\n",
    "print(f'Agent: {agent_action}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
